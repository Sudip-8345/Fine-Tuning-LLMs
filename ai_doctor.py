# -*- coding: utf-8 -*-
"""AI-Doctor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FfT9f-XLZ15MtxMX8_On7VddwD1QZBeA

1. colab
2. Hugging Face (Model hosting)
3. pyTorch
4. Weight & Biases (exp tracking)
5. Unsloth (Optimized fine tuning)
"""

!pip install unsloth
!pip install --no-deps "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

"""# **Fine Tuning**

### 1. Self Supervised
### 2. Reinforcement Learning
### 3. Supervised
   - Retrain all parameters
   - PEFT (Parameter based Fine Tuning)
     - LORA
"""

from unsloth import FastLanguageModel
import torch
from trl import SFTTrainer
from unsloth import is_bfloat16_supported
from huggingface_hub import login
from transformers import TrainingArguments
from datasets import load_dataset
import wandb

from google.colab import userdata
hf_token = userdata.get('HF_TOKEN')
login(hf_token)

import torch
print("CUDA available:", torch.cuda.is_available())
print("GPU device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")

# Step-5 : set up deepseep-R1
model_name = "unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit"
max_sequence_len = 2048
dtype = None
load_in_4bit = True

model = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length=max_sequence_len,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    token = hf_token
)

# Step6: Setup system prompt
prompt_style = """
Below is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.

Before crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.

### Task:
You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.

### Query:
{}

### Answer:
<think>{}
"""

# Step7: Run Inference on the model

tokenizer = model[1]
model = model[0]
# Define a test question
question = """A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or
              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,
              what would cystometry most likely reveal about her residual volume and detrusor contractions?"""

FastLanguageModel.for_inference(model)

# Tokenize the input
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Generate a response
outputs = model.generate(
    input_ids = inputs.input_ids,
    attention_mask = inputs.attention_mask,
    max_new_tokens = 1200,
    use_cache = True
)

# Decode the response tokens back to text
response = tokenizer.batch_decode(outputs)

print(response)

print(response[0].split("### Answer:")[1])

medical_dataset = load_dataset("FreedomIntelligence/medical-o1-reasoning-SFT",
                               "en",
                               split = "train[:1000]",
                               trust_remote_code = True
                  )

medical_dataset[0]

EOS_TOKEN = tokenizer.eos_token
EOS_TOKEN

### Finetuning
# Updated training prompt style to add  tag
train_prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.
Please answer the following medical question.

### Question:
{}

### Response:
<think>
{}
</think>
{}"""

# Prepare the data for fine-tuning

def formatting_prompts_func(examples):
    inputs  = examples["Question"]
    cots    = examples["Complex_CoT"]
    outputs = examples["Response"]

    texts = []
    EOS_TOKEN = tokenizer.eos_token # Ensure this is defined

    for input, cot, output in zip(inputs, cots, outputs):
        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN
        texts.append(text)

    return texts

finetune_dataset = medical_dataset.map(preprocess_input_data, batched = True)
finetune_dataset["texts"][0]

model_lora = FastLanguageModel.get_peft_model(
    model = model,
    r = 16,
    target_modules = [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = 'none',
    use_gradient_checkpointing = "unsloth",
    random_state = 3047,
    use_rslora = False,
    loftq_config=None
)

# Add this before creating the trainer
if hasattr(model, '_unwrapped_old_generate'):
    del model._unwrapped_old_generate

trainer = SFTTrainer(
    model = model_lora,
    tokenizer = tokenizer,
    train_dataset = medical_dataset,
    dataset_text_field = "texts",
    max_seq_length = max_sequence_len,
    dataset_num_proc = 1,

    formatting_func = formatting_prompts_func,

    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 10,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

# Setup WANDB
from google.colab import userdata
wnb_token = userdata.get("WANDB_API_KEY")
# Login to WnB
wandb.login(key=wnb_token)
run = wandb.init(
    project='Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset',
    job_type="training",
    anonymous="allow"
)

# Start the fine-tuning process
trainer_stats = trainer.train()

wandb.finish()

# Step10: Testing after fine-tuning
question = """A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing
              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,
              what would cystometry most likely reveal about her residual volume and detrusor contractions?"""

FastLanguageModel.for_inference(model_lora)

# Tokenize the input
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Generate a response
outputs = model_lora.generate (
    input_ids = inputs.input_ids,
    attention_mask = inputs.attention_mask,
    max_new_tokens = 1200,
    use_cache = True
)

# Decode the response tokens back to text
response = tokenizer.batch_decode(outputs)

print(response[0].split("### Answer:")[1])

question = """A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue,
              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative,
              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.
              What is the most likely predisposing factor for this patient's condition?"""

FastLanguageModel.for_inference(model_lora)

# Tokenize the input
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Generate a response
outputs = model_lora.generate (
    input_ids = inputs.input_ids,
    attention_mask = inputs.attention_mask,
    max_new_tokens = 1200,
    use_cache = True
)

# Decode the response tokens back to text
response = tokenizer.batch_decode(outputs)

print(response[0].split("### Answer:")[1])